@BOOK{latex04,
  AUTHOR =       {Frank Mittelbach and Michel Goossens},
  TITLE =        {The \LaTeX\ Companion},
  PUBLISHER =    {Addison-Wesley},
  YEAR =         {2004},
  address =      {Reading, MA, USA},
  edition =      {2nd}
}

@inproceedings{gregor2010learning,
  title={Learning fast approximations of sparse coding},
  author={Gregor, Karol and LeCun, Yann},
  booktitle={Proceedings of the 27th international conference on international conference on machine learning},
  pages={399--406},
  year={2010}
}

@inproceedings{xu2022gmflow,
  title={Gmflow: Learning optical flow via global matching},
  author={Xu, Haofei and Zhang, Jing and Cai, Jianfei and Rezatofighi, Hamid and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8121--8130},
  year={2022}
}

@article{diamond2017unrolled,
  title={Unrolled optimization with deep priors},
  author={Diamond, Steven and Sitzmann, Vincent and Heide, Felix and Wetzstein, Gordon},
  journal={arXiv preprint arXiv:1705.08041},
  year={2017}
}

@inproceedings{shi2023flowformer++,
  title={Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation},
  author={Shi, Xiaoyu and Huang, Zhaoyang and Li, Dasong and Zhang, Manyuan and Cheung, Ka Chun and See, Simon and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1599--1610},
  year={2023}
}

@article{saxena2024surprising,
  title={The surprising effectiveness of diffusion models for optical flow and monocular depth estimation},
  author={Saxena, Saurabh and Herrmann, Charles and Hur, Junhwa and Kar, Abhishek and Norouzi, Mohammad and Sun, Deqing and Fleet, David J},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yang2016deep,
  title={Deep ADMM-Net for compressive sensing MRI},
  author={Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben},
  booktitle={Proceedings of the 30th international conference on neural information processing systems},
  pages={10--18},
  year={2016}
}

@article{monga2021algorithm,
  title={Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing},
  author={Monga, Vishal and Li, Yuelong and Eldar, Yonina C},
  journal={IEEE Signal Processing Magazine},
  volume={38},
  number={2},
  pages={18--44},
  year={2021},
  publisher={IEEE}
}

@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={402--419},
  year={2020},
  organization={Springer}
}

@book{nikolenko2021synthetic,
	title={Synthetic data for deep learning},
	author={Nikolenko, Sergey I},
	volume={174},
	year={2021},
	publisher={Springer}
}

@article{chan2001active,
	title={Active contours without edges},
	author={Chan, Tony F and Vese, Luminita A},
	journal={IEEE Transactions on image processing},
	volume={10},
	number={2},
	pages={266--277},
	year={2001},
	publisher={IEEE}
}

@inproceedings{behera2021pidlnet,
	title={Pidlnet: A physics-induced deep learning network for characterization of crowd videos},
	author={Behera, Shreetam and Vijay, Thakare Kamalakar and Kausik, H Manish and Dogra, Debi Prosad},
	booktitle={2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},
	pages={1--8},
	year={2021},
	organization={IEEE}
}

@article{zheng2020physics,
	title={Physics-informed semantic inpainting: Application to geostatistical modeling},
	author={Zheng, Qiang and Zeng, Lingzao and Karniadakis, George Em},
	journal={Journal of Computational Physics},
	volume={419},
	pages={109676},
	year={2020},
	publisher={Elsevier}
}

@inproceedings{jenkins2020physics,
	title={Physics-informed detection and segmentation of type II solar radio bursts},
	author={Jenkins, Joseph and Paiement, Adeline and Aboudarham, Jean and Bonnin, Xavier},
	booktitle={British Machine Vision Virtual Conference},
	year={2020}
}

@inproceedings{cciccek20163d,
	title={3D U-Net: learning dense volumetric segmentation from sparse annotation},
	author={{\c{C}}i{\c{c}}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
	booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19},
	pages={424--432},
	year={2016},
	organization={Springer}
}

@article{arora2022spatio,
	title={Spatio-temporal super-resolution of dynamical systems using physics-informed deep-learning},
	author={Arora, Rajat and Shrivastava, Ankit},
	journal={arXiv preprint arXiv:2212.04457},
	year={2022}
}

@article{chu2022physics,
	title={Physics informed neural fields for smoke reconstruction with sparse data},
	author={Chu, Mengyu and Liu, Lingjie and Zheng, Quan and Franz, Erik and Seidel, Hans-Peter and Theobalt, Christian and Zayer, Rhaleb},
	journal={ACM Transactions on Graphics (TOG)},
	volume={41},
	number={4},
	pages={1--14},
	year={2022},
	publisher={ACM New York, NY, USA}
}

@inproceedings{geiger2012we,
	title={Are we ready for autonomous driving? the kitti vision benchmark suite},
	author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	booktitle={2012 IEEE conference on computer vision and pattern recognition},
	pages={3354--3361},
	year={2012},
	organization={IEEE}
}

@inproceedings{butler2012naturalistic,
	title={A naturalistic open source movie for optical flow evaluation},
	author={Butler, Daniel J and Wulff, Jonas and Stanley, Garrett B and Black, Michael J},
	booktitle={Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12},
	pages={611--625},
	year={2012},
	organization={Springer}
}

@inproceedings{sun2018pwc,
	title={Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume},
	author={Sun, Deqing and Yang, Xiaodong and Liu, Ming-Yu and Kautz, Jan},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={8934--8943},
	year={2018}
}

@article{baker2011database,
	title={A database and evaluation methodology for optical flow},
	author={Baker, Simon and Scharstein, Daniel and Lewis, JP and Roth, Stefan and Black, Michael J and Szeliski, Richard},
	journal={International journal of computer vision},
	volume={92},
	pages={1--31},
	year={2011},
	publisher={Springer}
}

@article{amiaz2006piecewise,
	title={Piecewise-smooth dense optical flow via level sets},
	author={Amiaz, Tomer and Kiryati, Nahum},
	journal={International Journal of Computer Vision},
	volume={68},
	pages={111--124},
	year={2006},
	publisher={Springer}
}

@article{horn1981determining,
	title={Determining optical flow},
	author={Horn, Berthold KP and Schunck, Brian G},
	journal={Artificial intelligence},
	volume={17},
	number={1-3},
	pages={185--203},
	year={1981},
	publisher={Elsevier}
}

@article{banerjee2023physics,
	title={Physics-Informed Computer Vision: A Review and Perspectives},
	author={Banerjee, Chayan and Nguyen, Kien and Fookes, Clinton and Karniadakis, George},
	journal={arXiv preprint arXiv:2305.18035},
	year={2023}
}

@article{raissi2018hidden,
	title={Hidden physics models: Machine learning of nonlinear partial differential equations},
	author={Raissi, Maziar and Karniadakis, George Em},
	journal={Journal of Computational Physics},
	volume={357},
	pages={125--141},
	year={2018},
	publisher={Elsevier}
}

@article{raissi2020hidden,
	title={Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
	author={Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
	journal={Science},
	volume={367},
	number={6481},
	pages={1026--1030},
	year={2020},
	publisher={American Association for the Advancement of Science}
}

@inproceedings{hui2018liteflownet,
	title={Liteflownet: A lightweight convolutional neural network for optical flow estimation},
	author={Hui, Tak-Wai and Tang, Xiaoou and Loy, Chen Change},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={8981--8989},
	year={2018}
}

@inproceedings{dosovitskiy2015flownet,
	title={Flownet: Learning optical flow with convolutional networks},
	author={Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and Hausser, Philip and Hazirbas, Caner and Golkov, Vladimir and Van Der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={2758--2766},
	year={2015}
}

@inproceedings{ilg2017flownet,
	title={Flownet 2.0: Evolution of optical flow estimation with deep networks},
	author={Ilg, Eddy and Mayer, Nikolaus and Saikia, Tonmoy and Keuper, Margret and Dosovitskiy, Alexey and Brox, Thomas},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2462--2470},
	year={2017}
}

@article{raissi2017physics,
	title={Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations},
	author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	journal={arXiv preprint arXiv:1711.10561},
	year={2017}
}

@article{raissi2019physics,
	title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
	journal={Journal of Computational physics},
	volume={378},
	pages={686--707},
	year={2019},
	publisher={Elsevier}
}

@article{lagaris1998artificial,
	title={Artificial neural networks for solving ordinary and partial differential equations},
	author={Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
	journal={IEEE transactions on neural networks},
	volume={9},
	number={5},
	pages={987--1000},
	year={1998},
	publisher={IEEE}
}

@article{lagaris1997artificial,
	title={Artificial neural network methods in quantum mechanics},
	author={Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
	journal={Computer Physics Communications},
	volume={104},
	number={1-3},
	pages={1--14},
	year={1997},
	publisher={Elsevier}
}

@article{grigo2019physics,
	title={A physics-aware, probabilistic machine learning framework for coarse-graining high-dimensional systems in the Small Data regime},
	author={Grigo, Constantin and Koutsourelakis, Phaedon-Stelios},
	journal={Journal of Computational Physics},
	volume={397},
	pages={108842},
	year={2019},
	publisher={Elsevier},
	annote = {
	Coarse grained models aim to simulate the behaviour of complex physical processes using simplified representations. These coarse models are typically smoothed representations of their fine-grained counterparts. Coarse graining is often necessary for providing simulations within a reasonable timeframe. In certain circumstances simulations of the fine grained counterparts are technically infeasible with current computing capabilities. It is desirable to have the resolution of the fine grain simulation with the computational demands of the coarse model. Accordingly, reconstruction/estimation of fine grained solutions using the coarse grained representation is of interest. However, there is inherit information loss due to the lower resolution of the surrogate model.
	
	The authors present a novel, fully Bayesian strategy for surrogate modeling designed for problems characterized by high input dimension and a small number of training data points. Specifically their framework can be broken into 3 main components; encoding for dimensionality reduction, model reduction from a fine to coarse grained model that either uses simplified physics or coarser resolution, and decoding used as a reconstructive step using a probabilistic map between the solved coarse projected to the fine grained model. Crucially this probabilistic encoding and decoding differs from classical dimensionality reduction as the encoded representation should be maximally predictive of the fine grain model output and not for the reconstruction of itself.
	
	To illustrate their methodology the authors leverage a physics-informed, fully Bayesian surrogate model to predict response fields of stochastic partial differential equations arising from random heterogeneous media. In particular they replaced an expensive Navier Stokes based fine grain simulation by an inexpensive, Bayesian, coarse-grained model built around the Darcy-flow skeleton simulating fluid flows in porous materials. This example has significant potential as usage for simulation and modelling of fluid flows in aquifiers and petroleum resovoirs. 
	
	The authors were able to show, through the similarity of the Darcy and Navier Stokes PDEs, numerical evidence of the predictive capabilities of their physics aware machine learning framework with R$\phantom{}^2$ scores of $0.995 \pm 0.004$ and a mean log likelihood of $-11.003 \pm 0.013$. Further they were able to demonstrate its ability to perform comparably well under extrapolative conditions showing strong generalisability.
	
	The authors suggested further work to explore implementation of a deep learning framework for coarse- graining of the high-dimensional stochastic inputs, potentially benefiting from recent advances in computer vision with e.g. convolutional neural networks (CNNs) or deep Gaussian process models.
	}
}

@article{burger2021connections,
	title={Connections between deep learning and partial differential equations},
	author={Burger, M and Ruthotto, L and OSHER, SJ and others},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={3},
	pages={395--396},
	year={2021},
	publisher={Cambridge University Press},
	annote = {
	The authors provide a short review on the connections between deep learning and partial differential equations. The intepretation of some deep neural networks as non linear partial differential equations allows for a reimagining of this problem space. By considering this new line of thought new deep learning architectures can be created. This short review highlights two main considerations. First, how deep learning can help to solve typical PDE related problems and second, the designing of novel schemes derived from PDE techniques. To illustrate the former, \citet{becker2021solving} utilised deep learning methodologies tackling optimal stopping problems for financial derivatives. Linearising arbitary non-linear PDEs are difficult and doing so analytically is impossible.  \citet{gin2021deep} utilized an autoencoder structure to learn mappings that transformed non-linear PDEs to linear PDEs, highlighting the depths role deep learning can take in such pure disciplines. Demonstrating the latter, \citet{chen2021learning} tackles forward and inverse problems with physics informed neural networks (PINN). In the numerical examples, the PINN approach accurately estimates both random and deterministic parameters of the system from a small number of measurements. They also demonstrate that Bayesian optimisation can automise the hyper-parameter tuning in PINNs. Further extending the connect of deep learning to PDEs; \citet{wang2021graph} replace the data-agnostic softmax function with graph-based interpolation to improve deep neural network classifiers’ accuracy and robustness. They show that this choice, in the continuum limit, converges to the Laplace–Beltrami equation on high- dimensional manifolds. Therefore, the work outlines new ways to combine advances in deep neural nets and manifold learning. 
	}
}

@article{becker2021solving,
	title={Solving high-dimensional optimal stopping problems using deep learning},
	author={Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Welti, Timo},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={3},
	pages={470--514},
	year={2021},
	publisher={Cambridge University Press},
	annote = {\toFix{TODO!}}
}

@article{gin2021deep,
	title={Deep learning models for global coordinate transformations that linearise PDEs},
	author={Gin, Craig and Lusch, Bethany and Brunton, Steven L and Kutz, J Nathan},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={3},
	pages={515--539},
	year={2021},
	publisher={Cambridge University Press},
	annote = {\toFix{TODO!}}
}
}

@article{chen2021learning,
	title={Learning and meta-learning of stochastic advection--diffusion--reaction systems from sparse measurements},
	author={Chen, Xiaoli and Duan, Jinqiao and Karniadakis, George Em},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={3},
	pages={397--420},
	year={2021},
	publisher={Cambridge University Press},
	annote = {\toFix{TODO!}}
}

@article{wang2021graph,
	title={Graph interpolating activation improves both natural and robust accuracies in data-efficient deep learning},
	author={Wang, Bao and Osher, Stan J},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={3},
	pages={540--569},
	year={2021},
	publisher={Cambridge University Press},
	annote = {\toFix{TODO!}}
}

@article{lin2008learning,
	title={Learning partial differential equations for computer vision},
	author={Lin, Zhouchen and Zhang, Wei and Tang, Xiaoou},
	journal={Peking Univ., Chin. Univ. of Hong Kong},
	year={2008},
	annote = {
	Partial differential equations (PDE) have been demonstrated to successfully solve a variety of computer vision problems. However, there exists some inhibating factors of their widespread adoption for these types of tasks. The PDEs require extensive hand crafting which is typically based from the designers intuitions. This has considerable drawbacks. PDEs become hard to hand design for tasks that are complex to describe such as object detection and image segmentation. The authors propose a framework that consists of a system of two PDEs. The first PDE is used to control the output given and input image, and the second is used as a global indicator function. The two PDEs are coupled and learned through optimal control techniques. This optimal control problem is derived from minimising a functional that contains a control acting on the function describing the evolution of the input image. Since the authors state this is preliminary work, they consider only second order PDEs. Further, they don't directly compare their results to what was state of the art results. Instead, they were able to show their system of learned PDEs produced analoguous results for a variety of computer vision problems. These problems included edge detection, image segmentation, image denoising and image blurring. From this the authors show the promise of learning PDEs for computer vision tasks allowing a reimagining of this problem domain.
	}
}

@article{raissi2017machine,
	title={Machine learning of linear differential equations using Gaussian processes},
	author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	journal={Journal of Computational Physics},
	volume={348},
	pages={683--693},
	year={2017},
	publisher={Elsevier},
	annote = {
	For fields such as science and engineering, differential and integro equations are seen as important describers of physical processes. There has been centuries worth of knowledge discovered with extreme explanatory power. Differential equations are the cornerstone of these disciplines. However, their use within statistics, probability theory and machine learning is less developed. Due to evermore connected IoT devices there are higher levels of data generation and consumption. Consequently, there is a move towards machine learning to gain valuable insights. These methods appear to disregard the advances made from the underlying physics models by employing black box universal approximators. There is a great challenge that can facilitate opportunities for considerable growth if frameworks can be developed to blend differential equations with machine learning, statistics and probability theory. 
	
	The author's work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. This work introduced a computational framework for learning general parametric linear equations from noisy data. They do so by encoding prior information using a kernel embedded in a Guassian process where maximisation of a marginal log likelihood takes place. Further, they consider the more generalised fractional differential operators. This blending of fractional calculus and machine learning is a major contribution of their work and it leads to an important observation that underpins the ability of the proposed framework to learn general linear differential operators from data. Their proposed framework allows the fractional degree to be directly inferred from noisy data, and opens the path to a flexible formalism for model discovery and calibration.
	
	Moreover, advantages can come from estimation of coefficients that are not known. These coefficients could be underpinned by a physical meaning, such as describing the rate of advection within a material. As such, inference over the systems characteristics can be made in a fast and principled manner.	
	}
}

@article{karniadakis2021physics,
	title={Physics-informed machine learning},
	author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	journal={Nature Reviews Physics},
	volume={3},
	number={6},
	pages={422--440},
	year={2021},
	publisher={Nature Publishing Group},
	annote = {
	Deep learning architectures normally require a large amount of data for training. This is due to the large number of parameters within the architecture that have to be regressed to find optimal values. In many physical problems there are a number of sources where noise can be introduced and it is often prohibitive to obtain the volume of data at the accuracy required. For these types of problems, physics-informed learning has the advantage of enforcing the embedded physics to effectively constrain the network' parameters on a lower-dimensional manifold. This restriction allows PINNs to be learn efficiently and effective in a small data regime. Of increased value, this embedding gives the learning architecture the ability to extrapolate it's predicts whilst maintaining high levels of accuracy. In addition to the enchanced trainability and generalisability, these physical principles are able to provide theoretical insights and elucidate the inner workings of deep learning.
	
	The authors provide an extensive review into the current developments of Physics informed learning. Within they detail particular uses for physics embedded machine learning models, the current and potential future developments and give some open questions. From this review some of the key points that become apparent are;
	\begin{itemize}
	\item Kernel based or neural network regression is effective for meshless implementations. This is important for using PINNs as a solver for forward problems that are typically solved with computationally expensive simulations.
	\item Physics informed Neural Networks are efficient for Ill posed and inverse problems and appear to be scalable.
	\item There are exciting new areas of research that appear to be promising. Operator-regression, search for new intrinsic varianbles and representations, equivariant neural networks with built in physical constraints are such examples. (NOTE: The learning of operators appears to be really interesting!)
	\item New frameworks and standardized benchmarks need to be developed to properly understand and develop robust novel physics informed architectures.
	\item There is a need to develop new mathematics for analysis and development of these architectures. (This is where our research could begin to be directed and this review identifies a clear knowledge gap)
	\end{itemize}
	
	Little is still known about the theoretical foundation when constraining neural networks when embedding the physical systems. New Theories have to be developed to rigorously analyse the limitations of these PINNs. Some of these limitations could be determining if gradient descent is sufficient for optimisation or stabliity of convergence. This is an exciting new area that should be explored. As an example, it would be nice to be able to determine the learning cability, stablity and understand how well posed a problem is based on a learnt operator.
	
	NOTE: There is a lot of information in this review! I need multiple reads to unpack it all. I want to write my own review in a similar style (informative, well written and comprehensive. The authors write about how to design these PINNs, their applications, some analysis and provide tool sets/ packages that are ready to go.)
	}
}

@article{brenner2021machine,
	title={Machine Learning for Partial Differential Equations},
	author={Brenner, Michael},
	journal={Bulletin of the American Physical Society},
	year={2021},
	publisher={APS},
	annote = {}
}

@article{celledoni2021structure,
	title={Structure-preserving deep learning},
	author={Celledoni, Elena and Ehrhardt, Matthias J and Etmann, Christian and McLachlan, Robert I and Owren, Brynjulf and SCHONLIEB, C-B and Sherry, Ferdia},
	journal={European Journal of Applied Mathematics},
	volume={32},
	number={5},
	pages={888--936},
	year={2021},
	publisher={Cambridge University Press},
	annote = { \toFix{
	Additionally, \citet{celledoni2021structure} provides an in-depth review of current work surrounding deep learning's association within multiple disciplines of applied and pure mathematics. This work links deep learning to key topological spaces, groups and algebras. One such example is by considering neural network architectures as discrete parametrisations of ordinary differential equations (ODEs) and PDEs. Multiple open questions are postulated by \citet{celledoni2021structure} surrounding deep learning's structure preservation and energy conservation abilities. Their open questions identify interesting mathematical avenues that could assist theoretical frameworks for systematic design and training of deep neural networks. 
	
	\paragraph{} By considering the statistical properties of the parameters to train, they can be modelled on an appropriate statistical manifold. Where a statistical manifold is equivalent to a Riemannian manifold with the points corresponding to probabilities. The standard stochastic gradient descent (SGD) for deep learning architectures can be implemented on statistical manifolds. This is studied within an area called \textit{informational geometry} \citep{celledoni2021structure}. The \textit{natural gradient} is the gradient on the statistical manifold. \cite{celledoni2021structure} summarises the link between the natural gradient and numerical approximations of the SGD algorithm using techniques such as Forward Euler, however makes the important distinction that analysis of the convergent properties of the analytical solutions are still an open problem. Further, and of stark importance, \citet{celledoni2021structure} state that metric gradient flows are typically used to study convergence properties of the network with an open problem of tying this idea within the informational geometry paradigm.
	}
	}
}

@article{li2020fourier,
	title={Fourier neural operator for parametric partial differential equations},
	author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	journal={arXiv preprint arXiv:2010.08895},
	year={2020},
	annote = {
	Many problems in science and engineering involve solving complex partial differential equation (PDE) systems repeatedly for different values of some parameters. Often such systems require fine discretization in order to capture the phenomenon being modeled.  In these situations numerical approximations to the solutions of Partial Differential Equations (PDEs) are cumbersome, slow and inefficient.
	
	Neural networks can be used as substitutes for numerical approximators of PDEs. However, they can only learn solutions at a particular discretisation. Development of mesh invarient neural networks architectures would provide substantial benefit. New work has proposed the concept of learning mesh-free, infinite dimensional operators. This is done with neural networks which have been consequently coined as \textit{neural operators}. Neural operators learn maps between infinite dimensional function spaces. Consequently, neural operators have the ability to learn an entire family of PDEs. In contrast, traditional numerical methods such as Runge Kutta, solve one instance. 
	
	The authors develop the Fourier neural operator which learns the operators in Fourier space. Within, they show that this neural operator learns the resolution-invariant solution operator for the family of Navier-Stokes equation in the turbulent regime. They claim this is the first work to do so. Further, they show that this neural operator is capable of performing zero-shot super-resolution, where it is trained on a lower resolution and directly evaluated on a higher resolution. This appears to be an analogue to the coarse graining methodology performed by \citet{karniadakis2021physics} for flows through porous materials. However, this is for the learned operator itself and not for the encoded solution approximation. The authors observed that the proposed framework can approximate complex operators. The operators they approximated were from highly non-linear PDEs, and they showed the operators could deal with high frequency modes and slow energy decay. 
	
	The authors stated verbatim that \\ \textit{Operator learning is not restricted to PDEs. Images can naturally be viewed as real-valued functions on 2-d domains and videos simply add a temporal structure. Our approach is therefore a natural choice for problems in computer vision where invariance to discretization crucial is important.} \\ \textbf{As such, operator learning should be an interesting avenue for us to explore. Especially a \textit{Fourier} or \textit{Wavelet Neural Operator.}}
	
	}
}

@article{nascimento2020tutorial,
	title={A tutorial on solving ordinary differential equations using Python and hybrid physics-informed neural network},
	author={Nascimento, Renato G and Fricke, Kajetan and Viana, Felipe AC},
	journal={Engineering Applications of Artificial Intelligence},
	volume={96},
	pages={103996},
	year={2020},
	publisher={Elsevier},
	annote = {
	The authors provide a tutorial for implementation of a hybrid neural network that implements physics based models. The authors leverage Tensorflow's custom layer architecture to create numerical integration cells for recurrent neural networks. They demonstrate the ability for these networks to solve two main problems. First, an ordinary differential equation that details fatigue crack propagation was solved using a forward euler numerical integrator cell in a RNN architecture. Second, a system of ODEs detailing forced vibrations of a 2 degree of freedom system were solved using a Runge Kutta numerical integrator cell in the RNN architecture. Both examples show the capabilities of PINNs to converge within a few training epochs on minimal data. 
	
	This work highlights the capabilities of current libraries such as tensorflow to easily integrate any novel architectures we design. Further it details, with code, examples on how to do so. With the numerical integration approach the authors take, there could be avenues to evaluate different numerical integrators such as Adam Bashforth. However, this might be a moot point as this technique appears to be similar to learning a numerical approximation kernel. More benefit would be gained by learning potentially more interesting operators/kernels. These operators could be linear like integration operators or highly non-linear (making use of non-linear activation funcitons). This observation may further encourage a deeper look into operator learning and kernel regression.
	}
}

@article{winovich2019convpde,
	title={ConvPDE-UQ: Convolutional neural networks with quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains},
	author={Winovich, Nick and Ramani, Karthik and Lin, Guang},
	journal={Journal of Computational Physics},
	volume={394},
	pages={263--279},
	year={2019},
	publisher={Elsevier},
	annote = {
	\toFix{
	The authors use a convolutional variational autoencoder as a numerical approximator for solving linear elliptic partial differential equations (Expand more on this paper). Bayesian approach, where the BNN (Bayesian Neural Network) is capable of performing uncertainty quantification.
	
	}
	}
}

@article{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@article{rudin1992nonlinear,
	title={Nonlinear total variation based noise removal algorithms},
	author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
	journal={Physica D: nonlinear phenomena},
	volume={60},
	number={1-4},
	pages={259--268},
	year={1992},
	publisher={Elsevier}
}

@inproceedings{brox2004high,
	title={High accuracy optical flow estimation based on a theory for warping},
	author={Brox, Thomas and Bruhn, Andr{\'e}s and Papenberg, Nils and Weickert, Joachim},
	booktitle={European conference on computer vision},
	pages={25--36},
	year={2004},
	organization={Springer}
}

@article{lu2019deeponet,
	title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	journal={arXiv preprint arXiv:1910.03193},
	year={2019}
}