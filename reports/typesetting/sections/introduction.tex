\section{Introduction}\label{sec:intro}

\IEEEPARstart{O}{ptical flow} refers to the pattern of apparent motion of objects in a sequence of images. Optical flow describes how pixels in an image move between consecutive frames, allowing for an understanding of the dynamics and motion of objects, the camera or both. Optical flow is a foundational concept within computer vison and is core for a wide variety of applications including; stereo imaging, depth map estimation, 3D scene reconstruction, frame interpolation, scene understanding, applications to autonomous vehicles and other machine vision applications.
\IEEEPARstart{}{} Importantly, optical flow allows for a sense of perceptual understanding and cognitive mapping through correlating visual flow to the motion of objects that are captured within a video feed. By accurately estimating optical flow, object movements can be tracked, changes can be detected, and an analysis of the flow of visual information can be conducted. Various applications can be enhanced with the additional spatiotemporal information, especially from highly dynamic scenes. A rudimentary example; estimations of the velocity and trajectory of an object of interest can be made allowing for decisions to be made by an autonomous system to avoid collisions.
\IEEEPARstart{}{} There are two divergent approaches to estimating dense optical flow. Classical methods, which include carefully handcrafted partial differential equations that are posed as an optimisation problem to solve. This leverages the framework of variational calculus where the phenomena is formulated as an energy functional to minimize. And deep learning approaches through deep neural networks, typically used to directly regress the optical flow field. 
\IEEEPARstart{}{} Convolutional Neural Networks (CNNs) have revolutionised the deep learning approach to optical flow estimation, and with it's success, shifted research focus from traditional approaches to deep learning. Errors within the estimations of the optical flow field appear to diffuse throughout the networks architecture as opposed to errors accumilating for traditional methods \cite{dosovitskiy2015flownet,sun2018pwc}. The current, state-of-the-art techniques, such as RAFT \cite{teed2020raft}, heavily rely on the CNN backbone, and nearly all leading methods integrate deep learning architectures into their frameworks. This paradigm shift has resulted in significant advancements in optical flow accuracy and performance, showcasing the effectiveness of deep learning's ability to capturing intricate motion patterns and improving flow estimation capabilities. 
\IEEEPARstart{} However, deep learning methodologies often require copious amounts of data to learn from. Collecting and annotating the large pools of data that is required to train these architectures is expensive and remains a major hurdle. Many of the benchmark datasets such as the KITTI \cite{geiger2012we}, MPI-Sintel \cite{butler2012naturalistic} and Middlebury \cite{baker2011database} collections have limited examples for robust deep learning models to be trained for real world applications. Due to the difficulty in annotating ground truth information, many of these benchmarking datasets are synthetically generated where significant gaps in performance between real world datasets exists. 
\IEEEPARstart{} The general trend for improving deep learning methods appears to be building larger and more complex architectures or by training on more data. If this trend continues, progress will be limited to only those with sufficent computational resources. Instead, this can be mitigated by intelligently exploiting the rules of the system, \textit{the physics}, and informing the deep learning architecture of this information. The conjecture is that Physics-Informed Neural Networks (PINNs) should provide substantial improvement to performance in a small data paradigm. PINNs can provide an interesting and attractive alternative to these complex models trained on large scaled datasets. PINNs have been shown to exhibit strong generalisation capabilities, across varied domains. This is particularly powerful where the physics is partially understood and with \textit{small} amounts of data to support the \textit{known} physics, justifying the idea \cite{raissi2017physics,raissi2017machine,grigo2019physics,karniadakis2021physics}.
\IEEEPARstart{}{} Algorithm unrolling, or unfolding, is a method where iterative algorithms are transformed into a chained series of the algorithm's primitive iterations. Each of the iterations becomes forward pass of a hidden layer within a neural network. Any of the parameters that are manually set in the traditional algorithm can be estimated throughout the unrolled variant, providing additional efficiency and flexibility. As a consequence, the unrolled layers in a neural network become intepretable and create a strong inductive bias. % LeCun and Gregor introduced the Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) as an unrolled version of ISTA \cite{gregor2010learning} obtaining better performance than the traditional ISTA with fewer iterations, opening up an end to end learning framework for classical iterative techniques. Subsequently, algorithm unrolling has increased in popularity, shown significant advances and strong performance in solving inverse imaging problems by merging deep learning architectures with traditional optimisation approaches \cite{monga2021algorithm,yang2016deep, diamond2017unrolled}.
\IEEEPARstart{}{} By unrolling algorithms that solve the physics of the system, an understanding of the governing rules can be directly embedded into the networks architecture. This allows the model to leverage the interpretability and structure of classical methods while enhancing them with the adaptability and data-driven insights of neural networks. This amalgamation not only improves the convergence and accuracy of the solutions but also provides a framework for integrating domain knowledge directly into neural networks, leading to more robust and generalizable models.
\IEEEPARstart{}{} In this work, we propose a novel physics-informed architecture that directly embeds knowledge of optical flow by unrolling a gradient descent optimization designed to solve optical flow. The integration of optical flow knowledge into the proposed deep learning architecture not only enhances generalization, particularly within a small data paradigm, but also facilitates elucidation of the model's estimation of flow patterns. \color{orange} modify this statement as we confirm/reject/ this hypothesis (or reveal another finding etc.) \color{black}.
\IEEEPARstart{}{} In the following section, section \ref{section:prelim} we provide preliminaries by presenting the theoretical foundations and mathematical tools that underpin optical flow. Section \ref{sec:related} presents the related works. Sections \ref{sec:method} and \ref{sec:experiment} show the methodology and the experiment design. Sections \ref{sec:results} and \ref{section:discussion} present the results and provide a discussion. Sections \ref{sec:conclusion} and \ref{sec:future} conclude and mention future directions for research to persue.