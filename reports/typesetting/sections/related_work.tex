
% There are challenges with accurately estimating dense optical flow. There are two mainstream techniques to estimating dense optical flow.

% 1. Traditional Methods - carefully handcrafted partial differential equations are posed as an optimisation problem to solve. This is typically done with variational calculus minimising an energy functional. This technique has seen fundamental advancements such as;
% • Horn-Schunck / Lucas-Kanade
% • Total Variation with L1 Regularisation (TV-L1)
% Or feature/keypoint tracking approaches, i.e. through usage of SIFT, to estimate and track the keypoints of interest.
% 2. Deep Learning approaches - end to end learning, hybrid approaches or convolutional neural networks (CNNs) are used to directly regress optical flow due to their ability to learn complex spatial temporal motion patterns. An instrumental work, FlowNet and it's later counterpart FlowNet2.0, showcased the power deep learning had for such optical flow estimation. 

% Deep learning, through leveraging the power CNNs has become the staple approach. One of the problems associated with this approach is the limited nature of the receptive field of the convolutional layers, whereby local correlations are only available,  posing an issue for large displacements.
% %
%
% \IEEEPARstart{}{} Convolutional Neural Networks (CNNs) have revolutionised the field of optical flow estimation, shifting research focus from traditional approaches to deep learning. Currently, state-of-the-art techniques heavily rely on CNNs, and nearly all leading methods integrate deep learning architectures into their frameworks. This paradigm shift has resulted in significant advancements in optical flow accuracy and performance, showcasing the effectiveness of deep learning's ability to capturing intricate motion patterns and improving flow estimation capabilities. However, deep learning methodologies often require copious amounts of data to learn from. Further, large architectures often have a large memory footprint making them infeasible for embedded systems. Collecting and annotating the large pools of data that is required to train these architectures is expensive and remains a major hurdle. Many of the benchmark datasets such as the KITTI \cite{geiger2012we}, MPI-Sintel \cite{butler2012naturalistic} and Middlebury \cite{baker2011database} collections have limited examples for robust models to be trained for real world applications. Most of these benchmarking datasets are synthetically generated, where there exists gaps in performance between realworld datasets such as KITTI \cite{geiger2012we}, HD1k \color{red}(ref)\color{black} and Middlebury \cite{baker2011database} compared to MPI-Sintel, Things \color{red}(ref)\color{black}, Flying Chairs \color{red}(ref)\color{black} and Virtual KITTI \color{red}(ref)\color{black}.

\section{Related Work}\label{sec:related}

\IEEEPARstart{}{} Optical flow estimation has been a long-standing research topic in computer vision, with various techniques contributing to its advancement. Due to it's foundational nature and importance, it is still of contemporary research interest \cite{xu2022gmflow,teed2020raft,shi2023flowformer++, saxena2024surprising}. Currently, there are two main approaches to estimate optical flow. Traditional techniques that typically leverage the calculus of variations, minimising a handcrafted energy functional. One of these foundational techniques, the Horn-Schunck method \cite{horn1981determining}, has been widely used for estimating optical flow by assuming brightness constancy and smoothness constraints to solve the aperture problem. More recently, deep learning-based approaches, such as FlowNet and its variants \cite{dosovitskiy2015flownet, ilg2017flownet, hui2018liteflownet, sun2018pwc}, have become the norm by leveraging CNNs to learn complex motion patterns directly from large-scale training data, leading to significant improvements in accuracy and robustness of optical flow estimation.

\IEEEPARstart{}{} Both variational and deep learning approaches suffer from distinct disadvantages. A major challenge for variational techniques is the assumption of brightness constancy, which assumes that the brightness of pixels remains constant between consecutive frames. In practice, variations in lighting conditions, occlusions, and motion blur can violate this assumption, leading to inaccuracies in flow estimation. Another challenge is the smoothness constraint, which assumes that neighbouring pixels have similar motion. This assumption may not hold in the presence of discontinuities, large displacements or complex motion patterns, leading to incorrect flow estimates. Motion discontinuities pose a challenge to dense optical flow algorithms. In variational optical flow methods, the excessive smoothing of flow discontinuities contributes significantly to the overall error \cite{amiaz2006piecewise}. The complexity of the energy functional needed to explain the variety of motion patterns to provide a generalisable solution inhibits application compared to deep learning techniques for accurate flow estimation \color{red}(is this statement too strong to mention without a reference?)\color{black}.

\IEEEPARstart{}{} Deep learning methods have been found to perform particularly well for large displacements and complex motion patterns, however, issues still exist \cite{ilg2017flownet}. Due to the size and complex architecture of deep learning architectures, they have a large number of parameters to learn causing a large memory footprint and require copious amounts of well labelled data learn from. The current datasets used for benchmarking; KITTI, MPI-Sintel and Middlebury do not have sufficient examples for a deep learning model to be robust for real world applications. Methods of generating synthetic optical flow examples have been used to bridge this gap, however, these data samples struggles to represent issues found in practice, such as motion blur and lighting variations \cite{nikolenko2021synthetic}.

\IEEEPARstart{}{} Physics informed learning methods have emerged as a promising approach in a wide variety of applications by integrating the underlying physical principles into the learning process. The broader research community has been lauding over the introduction of physics informed machine learning (PIML) methodologies since they were introduced by Raissi \cite{raissi2017physics}. Within their seminal paper, hidden fluid flows were learned by a PINN showcasing their power to generalise well in the presence of noisy and low fidelity information \cite{raissi2018hidden}. This spawned multiple branches for PIML to explore, most of which have found PIML methods have increased the powers of deep learning methods, especially in a small data paradigm \cite{karniadakis2021physics}. 

\IEEEPARstart{}{} Recently the term physics-informed computer vision (PICV) has been coined to describe the emerging domain of applying physics-informed models with computer vision tasks \cite{banerjee2023physics}. Physics-informed computer vision models exploit the benefits of having information gained from the underlying physics of the system and data to validate it's inference. The PICV domain is still in it's infancy and has many opportunities for rapid advancement. Examples of recent PICV applications include; superesolution \cite{arora2022spatio}, image generation \cite{zheng2020physics}, image reconstruction \cite{chu2022physics}, image segmentation \cite{jenkins2020physics, cciccek20163d} and crowd analysis \cite{behera2021pidlnet}. One such notable application includes inferring fluid flows from 4D-MRI imaging for the purposes of understanding arterial wall stress leveraging the well known Navier Stokes equations for incompressible fluid flows \cite{raissi2020hidden}. Estimations of the fluid flow vector field allows for the pressure and velocities to be estimated for advanced non-invasive diagnositcs for vulnerable patients. There is a strong link between \cite{raissi2020hidden} and learning of optical flow, however, it must be noted that as of yet we have not found any literature that directly learns an optical flow estimation using a PIML method. The links between physics-informed learning and algorithm unrolling are beginning to strengthen, and our work aids to justify the link between the two. With optimisation proceedures directly induced into the deep learning network, the argument is that the architecture can resolve the physics of the system and make effective use of this information to perform it's designed task.

\IEEEPARstart{}{} Algorithm unrolling has emerged as part of the contemporary computer vision research community and has impacted the intersection of iterative optimization techniques and deep learning, increasing both efficiency and model performance. The concept, pioneered by Gregor and LeCun with the introduction of the Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) \cite{gregor2010learning}, where this approach has been effectively employed in various domains, such as compressive sensing MRI \cite{yang2016deep}, and continues to evolve. Widespread adoption is evidenced in a review by Monga et al. \cite{monga2021algorithm}. The review demonstrated the effectiveness that unrolled optimisation algorithms have in solving inverse problems and hold substantial benefits for model explainability. Additional benefits are realised by inducing prior knowledge into deep learning architectures, fullfilling the inductive bias definition of physics-informed learning, allowing for a principled approach to modelling the problem space with deep learning architectures that have prior understanding. Our work builds on these advancements by proposing a novel physics-informed algorithm unrolled deep learning architecture, specifically tailored for optical flow estimation. By unrolling gradient descent and proximal gradient descent, our method embeds knowledge of the physical constraints directly into the learning process, aiming to achieve enhanced generalization and accuracy even in data-scarce scenarios.

% \color{red} Remove this paragraph from the related work? \color{gray} Due to the infancy of PICV, there are still many open problems left to explore. There is difficulty that surrounds the handcrafting of priors based on the imaging task. This often requires large amounts of domain expertise that crosses multiple areas, potentially causing an issue for widespread adoption. There is a marked importance in choosing the correct prior for the network to perform well. Additionally, appropriate benchmarking of performance is currently lacking, especially when comparing one PIML method to another. Complex architectures, such as UNET and RESNET that have been popularised by their success, encode their latent feature representation where the physical interpretation of these encodings have not been well understood. This creates an ambiguity if the architecture is learning meaningful representations. The effect of creating physics informed variants of these models therefore may prove challenging. Finally, some physics is \textit{intuitive} rather than can be described through a physical relationship. PIML architectures currently have no methodology to robustly incorporate this intuition that humans can infer. \color{black}

\IEEEPARstart{}{} There is a strong belief we are the first to perform optical flow estimation using an unrolled gradient descent physics-informed, deep learning method. As such, this work contributes a novel approach by leveraging the physics of optical flow, and the flexibility and robustness of deep learning. By directly influencing the deep learning architecture by inducing optical flow knowledge, the aim is to enhance the current capabilities of optical flow estimation while opening up new avenues for research and applications into the nascent field of physics-informed computer vision.